import { NextRequest, NextResponse } from 'next/server'

export interface ChatRequest {
  message: string
  mode: 'fast' | 'deep'
  chatHistory?: Array<{
    role: 'user' | 'assistant'
    content: string
  }>
}

export interface ChatResponse {
  content: string
  thinking?: string
  mode: 'fast' | 'deep'
  processingTime: number
  model: string
}

export async function POST(request: NextRequest) {
  try {
    const startTime = Date.now()
    const body: ChatRequest = await request.json()
    
    const { message, mode, chatHistory = [] } = body

    // –í–∞–ª–∏–¥–∞—Ü–∏—è –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    if (!message || !message.trim()) {
      return NextResponse.json(
        { error: '–°–æ–æ–±—â–µ–Ω–∏–µ –Ω–µ –º–æ–∂–µ—Ç –±—ã—Ç—å –ø—É—Å—Ç—ã–º' },
        { status: 400 }
      )
    }

    if (!['fast', 'deep'].includes(mode)) {
      return NextResponse.json(
        { error: '–ù–µ–≤–µ—Ä–Ω—ã–π —Ä–µ–∂–∏–º. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ "fast" –∏–ª–∏ "deep"' },
        { status: 400 }
      )
    }

    // –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞ –ò–ò
    const aiProvider = process.env.AI_PROVIDER || 'huggingface'
    
    let response: ChatResponse

    switch (aiProvider) {
      case 'huggingface':
        response = await handleHuggingFaceRequest(message, mode, chatHistory)
        break
      case 'ollama':
        response = await handleOllamaRequest(message, mode, chatHistory)
        break
      case 'openai':
        response = await handleOpenAIRequest(message, mode, chatHistory)
        break
      default:
        response = await handleMockRequest(message, mode, chatHistory)
    }

    response.processingTime = Date.now() - startTime

    return NextResponse.json(response)
    
  } catch (error) {
    console.error('–û—à–∏–±–∫–∞ API —á–∞—Ç–∞:', error)
    
    const errorMessage = error instanceof Error ? error.message : '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –æ—à–∏–±–∫–∞'
    
    return NextResponse.json(
      { 
        error: '–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∑–∞–ø—Ä–æ—Å–∞',
        details: process.env.NODE_ENV === 'development' ? errorMessage : undefined
      },
      { status: 500 }
    )
  }
}

// Hugging Face Inference API
async function handleHuggingFaceRequest(
  message: string, 
  mode: 'fast' | 'deep',
  chatHistory: Array<{ role: 'user' | 'assistant', content: string }>
): Promise<ChatResponse> {
  const HF_API_URL = 'https://api-inference.huggingface.co/models'
  const HF_TOKEN = process.env.HUGGINGFACE_TOKEN
  
  if (!HF_TOKEN) {
    throw new Error('HUGGINGFACE_TOKEN –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω')
  }

  // –í—ã–±–∏—Ä–∞–µ–º –º–æ–¥–µ–ª—å –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ä–µ–∂–∏–º–∞
  const model = mode === 'fast' 
    ? 'microsoft/DialoGPT-medium'
    : 'microsoft/DialoGPT-large'
  
  // –§–æ—Ä–º–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ –∏—Å—Ç–æ—Ä–∏–∏
  const context = chatHistory
    .slice(-10) // –ë–µ—Ä–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ 10 —Å–æ–æ–±—â–µ–Ω–∏–π
    .map(msg => `${msg.role === 'user' ? 'Human' : 'Assistant'}: ${msg.content}`)
    .join('\n')
  
  const prompt = context 
    ? `${context}\nHuman: ${message}\nAssistant:`
    : `Human: ${message}\nAssistant:`

  const response = await fetch(`${HF_API_URL}/${model}`, {
    method: 'POST',
    headers: {
      'Authorization': `Bearer ${HF_TOKEN}`,
      'Content-Type': 'application/json',
    },
    body: JSON.stringify({
      inputs: prompt,
      parameters: {
        max_new_tokens: mode === 'fast' ? 150 : 500,
        temperature: mode === 'fast' ? 0.7 : 0.8,
        do_sample: true,
        top_p: 0.9,
        return_full_text: false
      }
    })
  })

  if (!response.ok) {
    const errorData = await response.json().catch(() => ({}))
    throw new Error(`Hugging Face API error: ${response.status} - ${errorData.error || 'Unknown error'}`)
  }

  const result = await response.json()
  let content = result[0]?.generated_text || '–ò–∑–≤–∏–Ω–∏—Ç–µ, –Ω–µ —É–¥–∞–ª–æ—Å—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–≤–µ—Ç.'
  
  // –û—á–∏—â–∞–µ–º –æ—Ç–≤–µ—Ç –æ—Ç –ø—Ä–æ–º–ø—Ç–∞
  content = content.replace(prompt, '').trim()
  
  const chatResponse: ChatResponse = {
    content,
    mode,
    processingTime: 0, // –ë—É–¥–µ—Ç —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ –≤ –æ—Å–Ω–æ–≤–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–∏
    model: `HuggingFace: ${model}`
  }

  // –î–ª—è –≥–ª—É–±–æ–∫–æ–≥–æ —Ä–µ–∂–∏–º–∞ –¥–æ–±–∞–≤–ª—è–µ–º "–º—ã—à–ª–µ–Ω–∏–µ"
  if (mode === 'deep') {
    chatResponse.thinking = generateThinkingProcess(message, content)
  }

  return chatResponse
}

// Ollama (–ª–æ–∫–∞–ª—å–Ω—ã–π –∑–∞–ø—É—Å–∫)
async function handleOllamaRequest(
  message: string, 
  mode: 'fast' | 'deep',
  chatHistory: Array<{ role: 'user' | 'assistant', content: string }>
): Promise<ChatResponse> {
  const OLLAMA_URL = process.env.OLLAMA_URL || 'http://localhost:11434'
  
  // –í—ã–±–∏—Ä–∞–µ–º –º–æ–¥–µ–ª—å –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ä–µ–∂–∏–º–∞
  const model = mode === 'fast' ? 'llama2:7b' : 'llama2:13b'
  
  const messages = [
    ...chatHistory.slice(-10),
    { role: 'user', content: message }
  ]

  const response = await fetch(`${OLLAMA_URL}/api/chat`, {
    method: 'POST',
    headers: {
      'Content-Type': 'application/json',
    },
    body: JSON.stringify({
      model,
      messages,
      stream: false,
      options: {
        temperature: mode === 'fast' ? 0.7 : 0.8,
        top_p: 0.9,
        max_tokens: mode === 'fast' ? 150 : 500
      }
    })
  })

  if (!response.ok) {
    throw new Error(`Ollama API error: ${response.status}`)
  }

  const result = await response.json()
  const content = result.message?.content || '–ò–∑–≤–∏–Ω–∏—Ç–µ, –Ω–µ —É–¥–∞–ª–æ—Å—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–≤–µ—Ç.'

  const chatResponse: ChatResponse = {
    content,
    mode,
    processingTime: 0,
    model: `Ollama: ${model}`
  }

  if (mode === 'deep') {
    chatResponse.thinking = generateThinkingProcess(message, content)
  }

  return chatResponse
}

// OpenAI API (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
async function handleOpenAIRequest(
  message: string, 
  mode: 'fast' | 'deep',
  chatHistory: Array<{ role: 'user' | 'assistant', content: string }>
): Promise<ChatResponse> {
  const OPENAI_TOKEN = process.env.OPENAI_API_KEY
  
  if (!OPENAI_TOKEN) {
    throw new Error('OPENAI_API_KEY –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω')
  }

  const model = mode === 'fast' ? 'gpt-3.5-turbo' : 'gpt-4'
  
  const messages = [
    {
      role: 'system',
      content: mode === 'fast' 
        ? '–¢—ã –ø–æ–ª–µ–∑–Ω—ã–π –ò–ò-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç Komair. –û—Ç–≤–µ—á–∞–π –∫—Ä–∞—Ç–∫–æ –∏ —Ç–æ—á–Ω–æ.'
        : '–¢—ã –ø–æ–ª–µ–∑–Ω—ã–π –ò–ò-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç Komair. –ü—Ä–æ–≤–æ–¥–∏ –≥–ª—É–±–æ–∫–∏–π –∞–Ω–∞–ª–∏–∑ –≤–æ–ø—Ä–æ—Å–æ–≤ –∏ –¥–∞–≤–∞–π –ø–æ–¥—Ä–æ–±–Ω—ã–µ, –ø—Ä–æ–¥—É–º–∞–Ω–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã.'
    },
    ...chatHistory.slice(-10),
    { role: 'user', content: message }
  ]

  const response = await fetch('https://api.openai.com/v1/chat/completions', {
    method: 'POST',
    headers: {
      'Authorization': `Bearer ${OPENAI_TOKEN}`,
      'Content-Type': 'application/json',
    },
    body: JSON.stringify({
      model,
      messages,
      max_tokens: mode === 'fast' ? 150 : 500,
      temperature: mode === 'fast' ? 0.7 : 0.8,
    })
  })

  if (!response.ok) {
    throw new Error(`OpenAI API error: ${response.status}`)
  }

  const result = await response.json()
  const content = result.choices[0]?.message?.content || '–ò–∑–≤–∏–Ω–∏—Ç–µ, –Ω–µ —É–¥–∞–ª–æ—Å—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–≤–µ—Ç.'

  const chatResponse: ChatResponse = {
    content,
    mode,
    processingTime: 0,
    model: `OpenAI: ${model}`
  }

  if (mode === 'deep') {
    chatResponse.thinking = generateThinkingProcess(message, content)
  }

  return chatResponse
}

// –¢–µ—Å—Ç–æ–≤–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è (fallback)
async function handleMockRequest(
  message: string, 
  mode: 'fast' | 'deep',
  chatHistory: Array<{ role: 'user' | 'assistant', content: string }>
): Promise<ChatResponse> {
  // –ò–º–∏—Ç–∏—Ä—É–µ–º –∑–∞–¥–µ—Ä–∂–∫—É
  await new Promise(resolve => setTimeout(resolve, mode === 'fast' ? 500 : 2000))

  const responses = {
    fast: [
      `–ü–æ–Ω—è–ª –≤–∞—à –≤–æ–ø—Ä–æ—Å –æ "${message}". –≠—Ç–æ –±—ã—Å—Ç—Ä—ã–π –æ—Ç–≤–µ—Ç –æ—Ç Komair.`,
      `–û—Ç–≤–µ—á–∞—é –Ω–∞ –≤–∞—à –∑–∞–ø—Ä–æ—Å: "${message}". –í –±—ã—Å—Ç—Ä–æ–º —Ä–µ–∂–∏–º–µ —è –¥–∞—é –∫—Ä–∞—Ç–∫–∏–µ –æ—Ç–≤–µ—Ç—ã.`,
      `–í–∞—à –≤–æ–ø—Ä–æ—Å –æ "${message}" –æ–±—Ä–∞–±–æ—Ç–∞–Ω. –í–æ—Ç –∫—Ä–∞—Ç–∫–∏–π –æ—Ç–≤–µ—Ç.`
    ],
    deep: [
      `–ü–æ—Å–ª–µ —Ç—â–∞—Ç–µ–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –≤–∞—à–µ–≥–æ –≤–æ–ø—Ä–æ—Å–∞ "${message}", –º–æ–≥—É –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–∏—Ç—å –ø–æ–¥—Ä–æ–±–Ω–æ–µ –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ. –í —Ä–µ–∂–∏–º–µ –≥–ª—É–±–æ–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ —è —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—é –ø—Ä–æ–±–ª–µ–º—É —Å —Ä–∞–∑–Ω—ã—Ö —Å—Ç–æ—Ä–æ–Ω.`,
      `–ü—Ä–æ–≤–æ–¥—è –≥–ª—É–±–æ–∫–∏–π –∞–Ω–∞–ª–∏–∑ –≤–∞—à–µ–≥–æ –∑–∞–ø—Ä–æ—Å–∞ "${message}", —è —Ä–∞—Å—Å–º–æ—Ç—Ä–µ–ª –Ω–µ—Å–∫–æ–ª—å–∫–æ –∞—Å–ø–µ–∫—Ç–æ–≤ —ç—Ç–æ–π —Ç–µ–º—ã. –ü–æ–∑–≤–æ–ª—å—Ç–µ –¥–∞—Ç—å –≤–∞–º —Ä–∞–∑–≤–µ—Ä–Ω—É—Ç—ã–π –æ—Ç–≤–µ—Ç.`,
      `–í–∞—à –≤–æ–ø—Ä–æ—Å "${message}" —Ç—Ä–µ–±—É–µ—Ç –≤–Ω–∏–º–∞—Ç–µ–ª—å–Ω–æ–≥–æ —Ä–∞—Å—Å–º–æ—Ç—Ä–µ–Ω–∏—è. –í —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ –≥–ª—É–±–æ–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ —è –≥–æ—Ç–æ–≤ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–∏—Ç—å –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –æ—Ç–≤–µ—Ç.`
    ]
  }

  const content = responses[mode][Math.floor(Math.random() * responses[mode].length)]

  const chatResponse: ChatResponse = {
    content,
    mode,
    processingTime: 0,
    model: 'Mock API'
  }

  if (mode === 'deep') {
    chatResponse.thinking = generateThinkingProcess(message, content)
  }

  return chatResponse
}

// –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–æ—Ü–µ—Å—Å–∞ –º—ã—à–ª–µ–Ω–∏—è –¥–ª—è –≥–ª—É–±–æ–∫–æ–≥–æ —Ä–µ–∂–∏–º–∞
function generateThinkingProcess(question: string, answer: string): string {
  return `–ê–Ω–∞–ª–∏–∑ –≤–æ–ø—Ä–æ—Å–∞: "${question}"

üîç –®–∞–≥ 1: –ü–æ–Ω–∏–º–∞–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å —Å–ø—Ä–∞—à–∏–≤–∞–µ—Ç –æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π —Ç–µ–º–µ, –∫–æ—Ç–æ—Ä–∞—è —Ç—Ä–µ–±—É–µ—Ç –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ —Ä–∞—Å—Å–º–æ—Ç—Ä–µ–Ω–∏—è.

üß† –®–∞–≥ 2: –ê–Ω–∞–ª–∏–∑ –∫–ª—é—á–µ–≤—ã—Ö –∞—Å–ø–µ–∫—Ç–æ–≤
- –û–ø—Ä–µ–¥–µ–ª—è—é –æ—Å–Ω–æ–≤–Ω—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –≤–æ–ø—Ä–æ—Å–∞
- –†–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—é –≤–æ–∑–º–æ–∂–Ω—ã–µ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏
- –£—á–∏—Ç—ã–≤–∞—é –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏ –Ω—é–∞–Ω—Å—ã

üí° –®–∞–≥ 3: –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—Ç–≤–µ—Ç–∞
- –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –ª–æ–≥–∏—á–Ω–æ
- –ü—Ä–æ–≤–µ—Ä—è—é –ø–æ–ª–Ω–æ—Ç—É –∏ —Ç–æ—á–Ω–æ—Å—Ç—å
- –ê–¥–∞–ø—Ç–∏—Ä—É—é –ø–æ–¥ —É—Ä–æ–≤–µ–Ω—å —Å–ª–æ–∂–Ω–æ—Å—Ç–∏

‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç: –°—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞–Ω —Ä–∞–∑–≤–µ—Ä–Ω—É—Ç—ã–π –æ—Ç–≤–µ—Ç, —É—á–∏—Ç—ã–≤–∞—é—â–∏–π —Ä–∞–∑–ª–∏—á–Ω—ã–µ –∞—Å–ø–µ–∫—Ç—ã –≤–æ–ø—Ä–æ—Å–∞.`
}