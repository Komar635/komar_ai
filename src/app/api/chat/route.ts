
import { NextRequest, NextResponse } from 'next/server'
import { safeLogger } from '@/lib/logger'
import { providerManager } from '../../../lib/provider-manager'
import { responseCache } from '@/lib/response-cache'
import { createContextualPrompt, buildFinalPrompt } from '@/lib/prompts'

export interface ChatRequest {
  message: string
  mode: 'fast' | 'deep'
  chatHistory?: Array<{
    role: 'user' | 'assistant'
    content: string
  }>
}

export interface ChatResponse {
  content: string
  thinking?: string
  mode: 'fast' | 'deep'
  processingTime: number
  model: string
}

export async function POST(request: NextRequest) {
  try {
    const startTime = Date.now()
    const body: ChatRequest = await request.json()
    
    const { message, mode, chatHistory = [] } = body

    // –í–∞–ª–∏–¥–∞—Ü–∏—è –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    if (!message || !message.trim()) {
      return NextResponse.json(
        { error: '–°–æ–æ–±—â–µ–Ω–∏–µ –Ω–µ –º–æ–∂–µ—Ç –±—ã—Ç—å –ø—É—Å—Ç—ã–º' },
        { status: 400 }
      )
    }

    if (!['fast', 'deep'].includes(mode)) {
      return NextResponse.json(
        { error: '–ù–µ–≤–µ—Ä–Ω—ã–π —Ä–µ–∂–∏–º. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ "fast" –∏–ª–∏ "deep"' },
        { status: 400 }
      )
    }

    // –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à –ø–µ—Ä–µ–¥ –æ–±—Ä–∞—â–µ–Ω–∏–µ–º –∫ –ò–ò
    const cachedResponse = responseCache.get(message, mode)
    if (cachedResponse) {
      cachedResponse.processingTime = Date.now() - startTime
      return NextResponse.json(cachedResponse)
    }

    // –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ª—É—á—à–∏–π –¥–æ—Å—Ç—É–ø–Ω—ã–π –ø—Ä–æ–≤–∞–π–¥–µ—Ä
    let currentProvider = providerManager.getBestAvailableProvider()
    let response: ChatResponse
    let attemptCount = 0
    let lastError: Error | null = null
    
    safeLogger.info(`üöÄ –ù–∞—á–∏–Ω–∞–µ–º —Å –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞: ${currentProvider}`)

    // –ü—ã—Ç–∞–µ–º—Å—è –ø–æ–ª—É—á–∏—Ç—å –æ—Ç–≤–µ—Ç —Å fallback –º–µ–∂–¥—É –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞–º–∏
    while (currentProvider && currentProvider !== 'mock') {
      try {
        attemptCount++
        safeLogger.info(`üöÄ –ü–æ–ø—ã—Ç–∫–∞ ${attemptCount} —Å –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–º: ${currentProvider}`)
        
        response = await executeProviderRequest(currentProvider, message, mode, chatHistory)
        
        // –û—Ç–º–µ—á–∞–µ–º –ø—Ä–æ–≤–∞–π–¥–µ—Ä –∫–∞–∫ —Ä–∞–±–æ—Ç–æ—Å–ø–æ—Å–æ–±–Ω—ã–π
        providerManager.markProviderAsHealthy(currentProvider)
        
        // –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫—ç—à —É—Å–ø–µ—à–Ω—ã–π –æ—Ç–≤–µ—Ç
        responseCache.set(message, mode, response, currentProvider)
        
        response.processingTime = Date.now() - startTime
        return NextResponse.json(response)
        
      } catch (error) {
        lastError = error instanceof Error ? error : new Error(String(error))
        safeLogger.error(`‚ùå –û—à–∏–±–∫–∞ —Å –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–º ${currentProvider}:`, lastError.message)
        
        // –û—Ç–º–µ—á–∞–µ–º –ø—Ä–æ–≤–∞–π–¥–µ—Ä –∫–∞–∫ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã–π
        providerManager.markProviderAsUnhealthy(currentProvider, lastError.message)
        
        // –ü—Ä–æ–≤–µ—Ä—è–µ–º, –º–æ–∂–Ω–æ –ª–∏ –ø–æ–≤—Ç–æ—Ä–∏—Ç—å —Å —Ç–µ–∫—É—â–∏–º –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–º
        if (providerManager.canRetryWithProvider(currentProvider, attemptCount)) {
          safeLogger.info(`üîÑ –ü–æ–≤—Ç–æ—Ä–Ω–∞—è –ø–æ–ø—ã—Ç–∫–∞ —Å ${currentProvider}...`)
          continue
        }
        
        // –ü–µ—Ä–µ–∫–ª—é—á–∞–µ–º—Å—è –Ω–∞ —Å–ª–µ–¥—É—é—â–∏–π –ø—Ä–æ–≤–∞–π–¥–µ—Ä
        const nextProvider = providerManager.getNextProvider(currentProvider)
        if (nextProvider) {
          safeLogger.info(`üîÑ –ü–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ –Ω–∞ —Ä–µ–∑–µ—Ä–≤–Ω—ã–π –ø—Ä–æ–≤–∞–π–¥–µ—Ä: ${nextProvider}`)
          currentProvider = nextProvider
          attemptCount = 0
          continue
        }
        
        break // –í—Å–µ –ø—Ä–æ–≤–∞–π–¥–µ—Ä—ã –∏—Å—á–µ—Ä–ø–∞–Ω—ã
      }
    }
    
    // –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê: –ï—Å–ª–∏ –º—ã –∑–¥–µ—Å—å - –∑–Ω–∞—á–∏—Ç –í–°–ï —Ä–µ–∞–ª—å–Ω—ã–µ –ø—Ä–æ–≤–∞–π–¥–µ—Ä—ã –Ω–µ —Ä–∞–±–æ—Ç–∞—é—Ç!
    // –≠—Ç–æ –Ω–µ –¥–æ–ª–∂–Ω–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç—å —Å –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–Ω—ã–º Groq API
    safeLogger.error('üö® –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê: –í—Å–µ —Ä–µ–∞–ª—å–Ω—ã–µ –ø—Ä–æ–≤–∞–π–¥–µ—Ä—ã –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã!');
    safeLogger.error('üîß –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ API –∫–ª—é—á–µ–π –≤ .env.local');
    safeLogger.error('üîç –ü–æ—Å–ª–µ–¥–Ω—è—è –æ—à–∏–±–∫–∞:', lastError?.message || '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞');
    
    // –í–æ–∑–≤—Ä–∞—â–∞–µ–º –¥–∏–∞–≥–Ω–æ—Å—Ç–∏—á–µ—Å–∫—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫—É
    const diagnosticInfo = {
      availableProviders: providerManager.getProvidersStatus().map(p => ({ 
        name: p.name, 
        healthy: p.isHealthy, 
        error: p.lastError 
      })),
      envVars: {
        groqKey: process.env.GROQ_API_KEY ? '‚úÖ –ù–∞—Å—Ç—Ä–æ–µ–Ω' : '‚ùå –û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç',
        hfToken: process.env.HUGGINGFACE_TOKEN ? '‚úÖ –ù–∞—Å—Ç—Ä–æ–µ–Ω' : '‚ùå –û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç',
        togetherKey: process.env.TOGETHER_API_KEY ? '‚úÖ –ù–∞—Å—Ç—Ä–æ–µ–Ω' : '‚ùå –û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç'
      },
      lastError: lastError?.message
    };
    
    return NextResponse.json({
      error: '–í—Å–µ AI –ø—Ä–æ–≤–∞–π–¥–µ—Ä—ã –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã',
      message: '–ü—Ä–æ–≤–µ—Ä—å—Ç–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ API –∫–ª—é—á–µ–π –∏ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç—É',
      diagnostics: process.env.NODE_ENV === 'development' ? diagnosticInfo : undefined
    }, { status: 503 });
    
  } catch (error) {
    safeLogger.error('–û—à–∏–±–∫–∞ API —á–∞—Ç–∞:', error)
    
    const errorMessage = error instanceof Error ? error.message : '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –æ—à–∏–±–∫–∞'
    const errorStack = error instanceof Error ? error.stack : '–ù–µ—Ç —Å—Ç–µ–∫–∞ –æ—à–∏–±–æ–∫'
    
    // –ü–æ–¥—Ä–æ–±–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ–± –æ—à–∏–±–∫–µ –¥–ª—è —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–∞
    safeLogger.error(`–ü–æ–¥—Ä–æ–±–Ω–æ—Å—Ç–∏ –æ—à–∏–±–∫–∏: ${errorMessage}`)
    safeLogger.error(`–°—Ç–µ–∫ –æ—à–∏–±–∫–∏: ${errorStack}`)
    
    // –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤ –¥–ª—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏
    const providersStatus = providerManager.getProvidersStatus()
    safeLogger.error('–°—Ç–∞—Ç—É—Å –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤:', providersStatus)
    
    return NextResponse.json(
      { 
        error: '–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∑–∞–ø—Ä–æ—Å–∞',
        details: process.env.NODE_ENV === 'development' ? {
          message: errorMessage,
          stack: errorStack,
          providersStatus
        } : undefined
      },
      { status: 500 }
    )
  }
}

/**
 * –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∑–∞–ø—Ä–æ—Å–∞ –∫ –ø—Ä–æ–≤–∞–π–¥–µ—Ä—É
 */
async function executeProviderRequest(
  provider: string,
  message: string, 
  mode: 'fast' | 'deep',
  chatHistory: Array<{ role: 'user' | 'assistant', content: string }>
): Promise<ChatResponse> {
  switch (provider) {
    case 'huggingface':
      return await handleHuggingFaceRequest(message, mode, chatHistory)
    case 'ollama':
      return await handleOllamaRequest(message, mode, chatHistory)
    case 'together':
      return await handleTogetherAIRequest(message, mode, chatHistory)
    case 'groq':
      return await handleGroqRequest(message, mode, chatHistory)
    case 'cohere':
      return await handleCohereRequest(message, mode, chatHistory)
    case 'openai':
      return await handleOpenAIRequest(message, mode, chatHistory)
    case 'mock':
    default:
      return await handleMockRequest(message, mode, chatHistory)
  }
}

// Hugging Face Inference API
async function handleHuggingFaceRequest(
  message: string, 
  mode: 'fast' | 'deep',
  chatHistory: Array<{ role: 'user' | 'assistant', content: string }>
): Promise<ChatResponse> {
  const HF_API_URL = 'https://api-inference.huggingface.co/models'
  const HF_TOKEN = process.env.HUGGINGFACE_TOKEN
  
  if (!HF_TOKEN) {
    // –ï—Å–ª–∏ —Ç–æ–∫–µ–Ω –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º –±–µ—Å–ø–ª–∞—Ç–Ω—ã–µ –º–æ–¥–µ–ª–∏ –±–µ–∑ –∞—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏
    safeLogger.warn('HUGGINGFACE_TOKEN –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –±–µ—Å–ø–ª–∞—Ç–Ω—ã–π –¥–æ—Å—Ç—É–ø')
  }

  // –ë–µ—Å–ø–ª–∞—Ç–Ω—ã–µ –º–æ–¥–µ–ª–∏ Hugging Face
  const models = {
    fast: [
      'microsoft/DialoGPT-medium',
      'facebook/blenderbot-400M-distill',
      'microsoft/DialoGPT-small'
    ],
    deep: [
      'microsoft/DialoGPT-large', 
      'facebook/blenderbot-1B-distill',
      'google/flan-t5-base'
    ]
  }
  
  const selectedModels = models[mode]
  let lastError: Error | null = null
  
  // –ü—Ä–æ–±—É–µ–º –º–æ–¥–µ–ª–∏ –ø–æ –æ—á–µ—Ä–µ–¥–∏ –¥–æ —É—Å–ø–µ—à–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞
  for (const model of selectedModels) {
    try {
      safeLogger.info(`–ü—Ä–æ–±—É–µ–º –º–æ–¥–µ–ª—å: ${model}`)
      
      // –§–æ—Ä–º–∏—Ä—É–µ–º –∑–∞–ø—Ä–æ—Å –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–∏–ø–∞ –º–æ–¥–µ–ª–∏
      let payload: any
      
      if (model.includes('DialoGPT')) {
        // –î–ª—è DialoGPT —Ñ–æ—Ä–º–∏—Ä—É–µ–º –¥–∏–∞–ª–æ–≥–æ–≤—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç
        const context = chatHistory
          .slice(-5) // –ë–µ—Ä–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ 5 —Å–æ–æ–±—â–µ–Ω–∏–π –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
          .map(msg => `${msg.role === 'user' ? 'Human' : 'Bot'}: ${msg.content}`)
          .join(' ')
        
        const prompt = context 
          ? `${context} Human: ${message} Bot:`
          : `Human: ${message} Bot:`
          
        payload = {
          inputs: prompt,
          parameters: {
            max_new_tokens: mode === 'fast' ? 100 : 200,
            temperature: 0.7,
            do_sample: true,
            top_p: 0.9,
            return_full_text: false,
            pad_token_id: 50256
          },
          options: {
            wait_for_model: true,
            use_cache: false
          }
        }
      } else if (model.includes('blenderbot')) {
        // –î–ª—è BlenderBot –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ—Å—Ç–æ–π —Ç–µ–∫—Å—Ç
        payload = {
          inputs: message,
          parameters: {
            max_length: mode === 'fast' ? 100 : 200,
            min_length: 10,
            do_sample: true,
            temperature: 0.7
          },
          options: {
            wait_for_model: true,
            use_cache: false
          }
        }
      } else {
        // –î–ª—è T5 –∏ –¥—Ä—É–≥–∏—Ö –º–æ–¥–µ–ª–µ–π
        payload = {
          inputs: `–û—Ç–≤–µ—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å: ${message}`,
          parameters: {
            max_new_tokens: mode === 'fast' ? 100 : 200,
            temperature: 0.7
          },
          options: {
            wait_for_model: true,
            use_cache: false
          }
        }
      }
      
      const headers: Record<string, string> = {
        'Content-Type': 'application/json',
      }
      
      if (HF_TOKEN) {
        headers['Authorization'] = `Bearer ${HF_TOKEN}`
      }

      const response = await fetch(`${HF_API_URL}/${model}`, {
        method: 'POST',
        headers,
        body: JSON.stringify(payload)
      })

      if (!response.ok) {
        const errorData = await response.json().catch(() => ({}))
        throw new Error(`HTTP ${response.status}: ${errorData.error || response.statusText}`)
      }

      const result = await response.json()
      
      let content: string
      
      if (Array.isArray(result) && result[0]) {
        if (result[0].generated_text !== undefined) {
          content = result[0].generated_text
          // –û—á–∏—â–∞–µ–º –æ—Ç –∏—Å—Ö–æ–¥–Ω–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞ –¥–ª—è DialoGPT
          if (model.includes('DialoGPT')) {
            content = content.split('Bot:').pop()?.trim() || content
          }
        } else if (result[0].translation_text) {
          content = result[0].translation_text
        } else {
          content = result[0].summary_text || JSON.stringify(result[0])
        }
      } else {
        content = result.generated_text || result.text || '–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –æ—Ç–≤–µ—Ç –æ—Ç –º–æ–¥–µ–ª–∏.'
      }
      
      // –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞—á–µ—Å—Ç–≤–æ –æ—Ç–≤–µ—Ç–∞
      if (!content || content.trim().length < 5) {
        throw new Error('–°–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π –æ—Ç–≤–µ—Ç –æ—Ç –º–æ–¥–µ–ª–∏')
      }
      
      const chatResponse: ChatResponse = {
        content: content.trim(),
        mode,
        processingTime: 0,
        model: `HuggingFace: ${model}`
      }

      // –î–ª—è –≥–ª—É–±–æ–∫–æ–≥–æ —Ä–µ–∂–∏–º–∞ –¥–æ–±–∞–≤–ª—è–µ–º "–º—ã—à–ª–µ–Ω–∏–µ"
      if (mode === 'deep') {
        chatResponse.thinking = generateThinkingProcess(message, content)
      }

      safeLogger.info(`–£—Å–ø–µ—à–Ω—ã–π –æ—Ç–≤–µ—Ç –æ—Ç –º–æ–¥–µ–ª–∏: ${model}`)
      return chatResponse
      
    } catch (error) {
      safeLogger.error(`–û—à–∏–±–∫–∞ —Å –º–æ–¥–µ–ª—å—é ${model}:`, error)
      lastError = error instanceof Error ? error : new Error(String(error))
      
      // –ï—Å–ª–∏ —ç—Ç–æ –æ—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏, –∂–¥–µ–º –Ω–µ–º–Ω–æ–≥–æ
      if (lastError.message.includes('loading')) {
        safeLogger.info('–ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è, –∂–¥–µ–º 3 —Å–µ–∫—É–Ω–¥—ã...')
        await new Promise(resolve => setTimeout(resolve, 3000))
      }
      
      continue // –ü—Ä–æ–±—É–µ–º —Å–ª–µ–¥—É—é—â—É—é –º–æ–¥–µ–ª—å
    }
  }
  
  // –ï—Å–ª–∏ –≤—Å–µ –º–æ–¥–µ–ª–∏ –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª–∏, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø–æ–ª–µ–∑–Ω—ã–π fallback
  safeLogger.error('–í—Å–µ –º–æ–¥–µ–ª–∏ Hugging Face –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã, –∏—Å–ø–æ–ª—å–∑—É–µ–º —É–ª—É—á—à–µ–Ω–Ω—ã–π fallback')
  
  // –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –ø–æ–ª–µ–∑–Ω—ã–π –æ—Ç–≤–µ—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
  let content = generateUniversalResponse(message, mode)
  
  return {
    content,
    mode,
    processingTime: 0,
    model: 'Komair: Smart Fallback'
  }
}

// Ollama (–ª–æ–∫–∞–ª—å–Ω—ã–π –∑–∞–ø—É—Å–∫)
async function handleOllamaRequest(
  message: string, 
  mode: 'fast' | 'deep',
  chatHistory: Array<{ role: 'user' | 'assistant', content: string }>
): Promise<ChatResponse> {
  const OLLAMA_URL = process.env.OLLAMA_URL || 'http://localhost:11434'
  
  // –í—ã–±–∏—Ä–∞–µ–º Llama –º–æ–¥–µ–ª—å –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ä–µ–∂–∏–º–∞
  const model = mode === 'fast' ? 'llama3:8b' : 'llama3:70b'
  
  const messages = [
    {
      role: 'system',
      content: createContextualPrompt(mode)
    },
    ...chatHistory.slice(-8),
    { role: 'user', content: message }
  ]

  safeLogger.info(`ü¶ô –ò—Å–ø–æ–ª—å–∑—É–µ–º –ª–æ–∫–∞–ª—å–Ω—É—é Llama: ${model}`)

  const response = await fetch(`${OLLAMA_URL}/api/chat`, {
    method: 'POST',
    headers: {
      'Content-Type': 'application/json',
    },
    body: JSON.stringify({
      model,
      messages,
      stream: false,
      options: {
        temperature: mode === 'fast' ? 0.7 : 0.8,
        top_p: 0.9,
        num_predict: mode === 'fast' ? 200 : 500
      }
    })
  })

  if (!response.ok) {
    throw new Error(`Ollama API error: ${response.status}. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ, —á—Ç–æ Ollama –∑–∞–ø—É—â–µ–Ω –Ω–∞ ${OLLAMA_URL}`)
  }

  const result = await response.json()
  const content = result.message?.content || '–ò–∑–≤–∏–Ω–∏—Ç–µ, –Ω–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –æ—Ç–≤–µ—Ç –æ—Ç Llama.'

  const chatResponse: ChatResponse = {
    content: content.trim(),
    mode,
    processingTime: 0,
    model: `Ollama: ${model}`
  }

  if (mode === 'deep') {
    chatResponse.thinking = generateThinkingProcess(message, content)
  }

  return chatResponse
}

// Together AI (–±–µ—Å–ø–ª–∞—Ç–Ω—ã–µ Llama –º–æ–¥–µ–ª–∏)
async function handleTogetherAIRequest(
  message: string, 
  mode: 'fast' | 'deep',
  chatHistory: Array<{ role: 'user' | 'assistant', content: string }>
): Promise<ChatResponse> {
  const TOGETHER_API_URL = 'https://api.together.xyz/v1/chat/completions'
  const TOGETHER_TOKEN = process.env.TOGETHER_API_KEY
  
  if (!TOGETHER_TOKEN) {
    throw new Error('TOGETHER_API_KEY –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω. –ü–æ–ª—É—á–∏—Ç–µ –±–µ—Å–ø–ª–∞—Ç–Ω—ã–π —Ç–æ–∫–µ–Ω –Ω–∞ https://api.together.xyz')
  }

  // –í—ã–±–∏—Ä–∞–µ–º Llama –º–æ–¥–µ–ª—å –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ä–µ–∂–∏–º–∞
  const model = mode === 'fast' 
    ? 'meta-llama/Llama-2-7b-chat-hf'  // –ë—ã—Å—Ç—Ä–∞—è –º–æ–¥–µ–ª—å
    : 'meta-llama/Llama-2-13b-chat-hf' // –ë–æ–ª–µ–µ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å
  
  const messages = [
    {
      role: 'system',
      content: createContextualPrompt(mode)
    },
    ...chatHistory.slice(-6),
    { role: 'user', content: message }
  ]

  safeLogger.info(`ü¶ô –ò—Å–ø–æ–ª—å–∑—É–µ–º Llama –º–æ–¥–µ–ª—å: ${model}`)

  const response = await fetch(TOGETHER_API_URL, {
    method: 'POST',
    headers: {
      'Authorization': `Bearer ${TOGETHER_TOKEN}`,
      'Content-Type': 'application/json',
    },
    body: JSON.stringify({
      model,
      messages,
      max_tokens: mode === 'fast' ? 200 : 500,
      temperature: mode === 'fast' ? 0.7 : 0.8,
      top_p: 0.9,
      repetition_penalty: 1.1
    })
  })

  if (!response.ok) {
    const errorData = await response.json().catch(() => ({}))
    throw new Error(`Together AI error: ${response.status} - ${errorData.error?.message || 'Unknown error'}`)
  }

  const result = await response.json()
  const content = result.choices[0]?.message?.content || '–ò–∑–≤–∏–Ω–∏—Ç–µ, –Ω–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –æ—Ç–≤–µ—Ç –æ—Ç Llama –º–æ–¥–µ–ª–∏.'

  const chatResponse: ChatResponse = {
    content: content.trim(),
    mode,
    processingTime: 0,
    model: `Together AI: ${model}`
  }

  if (mode === 'deep') {
    chatResponse.thinking = generateThinkingProcess(message, content)
  }

  return chatResponse
}

// Groq (—Å–≤–µ—Ä—Ö–±—ã—Å—Ç—Ä—ã–µ Llama –º–æ–¥–µ–ª–∏)
async function handleGroqRequest(
  message: string, 
  mode: 'fast' | 'deep',
  chatHistory: Array<{ role: 'user' | 'assistant', content: string }>
): Promise<ChatResponse> {
  const GROQ_API_URL = 'https://api.groq.com/openai/v1/chat/completions'
  const GROQ_TOKEN = process.env.GROQ_API_KEY
  
  if (!GROQ_TOKEN) {
    throw new Error('GROQ_API_KEY –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω. –ü–æ–ª—É—á–∏—Ç–µ –±–µ—Å–ø–ª–∞—Ç–Ω—ã–π —Ç–æ–∫–µ–Ω –Ω–∞ https://console.groq.com')
  }

  // Groq —Å—Ç–∞–±–∏–ª—å–Ω—ã–µ —Ä–∞–±–æ—á–∏–µ –º–æ–¥–µ–ª–∏ (–ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã–µ)
  const model = mode === 'fast' 
    ? 'llama-3.1-8b-instant'    // –ë—ã—Å—Ç—Ä–∞—è –º–æ–¥–µ–ª—å - —Ä–∞–±–æ—Ç–∞–µ—Ç —Å—Ç–∞–±–∏–ª—å–Ω–æ
    : 'llama-3.1-8b-instant'    // –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç—É –∂–µ –±—ã—Å—Ç—Ä—É—é –º–æ–¥–µ–ª—å –¥–ª—è –≥–ª—É–±–æ–∫–æ–≥–æ —Ä–µ–∂–∏–º–∞
  
  const messages = [
    {
      role: 'system',
      content: createContextualPrompt(mode)
    },
    ...chatHistory.slice(-6),
    { role: 'user', content: message }
  ]

  safeLogger.info(`‚ö° –ò—Å–ø–æ–ª—å–∑—É–µ–º Groq Llama: ${model}`)
  safeLogger.info(`üîë –ö–ª—é—á API: ${GROQ_TOKEN ? '–ù–∞—Å—Ç—Ä–æ–µ–Ω (' + GROQ_TOKEN.substring(0, 10) + '...)' : '–ù–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω'}`)
  safeLogger.info(`üì§ –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∑–∞–ø—Ä–æ—Å –≤ Groq API...`)
  
  const requestPayload = {
    model,
    messages,
    max_tokens: mode === 'fast' ? 200 : 500,
    temperature: mode === 'fast' ? 0.7 : 0.8,
    top_p: 0.9
  }
  
  safeLogger.info(`üìã Payload:`, { 
    model: requestPayload.model, 
    messagesCount: requestPayload.messages.length,
    maxTokens: requestPayload.max_tokens 
  })

  const response = await fetch(GROQ_API_URL, {
    method: 'POST',
    headers: {
      'Authorization': `Bearer ${GROQ_TOKEN}`,
      'Content-Type': 'application/json',
    },
    body: JSON.stringify(requestPayload)
  })
  
  safeLogger.info(`üì• Groq –æ—Ç–≤–µ—Ç: ${response.status} ${response.statusText}`)

  if (!response.ok) {
    const errorData = await response.json().catch(() => ({}))
    
    safeLogger.error(`Groq API –æ—à–∏–±–∫–∞: ${response.status}`, errorData)
    
    // –°–ø–µ—Ü–∏–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ cfToken –æ—à–∏–±–∫–∏
    const errorMessage = errorData.error?.message || errorData.message || JSON.stringify(errorData)
    if (errorMessage.includes('cfToken')) {
      throw new Error(`Groq –∞—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏—è –Ω–µ —É–¥–∞–ª–∞—Å—å (cfToken error). –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å GROQ_API_KEY –≤ .env.local`)
    }
    
    // –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥—Ä—É–≥–∏—Ö –æ—à–∏–±–æ–∫ –∞—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏
    if (response.status === 401) {
      throw new Error(`Groq API: –ù–µ–≤–µ—Ä–Ω—ã–π API –∫–ª—é—á. –ü–æ–ª—É—á–∏—Ç–µ –Ω–æ–≤—ã–π –∫–ª—é—á –Ω–∞ https://console.groq.com`)
    }
    
    if (response.status === 429) {
      throw new Error(`Groq API: –ü—Ä–µ–≤—ã—à–µ–Ω –ª–∏–º–∏—Ç –∑–∞–ø—Ä–æ—Å–æ–≤. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ.`)
    }
    
    throw new Error(`Groq API error: ${response.status} - ${errorData.error?.message || errorData.message || 'Unknown error'}`)
  }

  const result = await response.json()
  const content = result.choices[0]?.message?.content || '–ò–∑–≤–∏–Ω–∏—Ç–µ, –Ω–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –æ—Ç–≤–µ—Ç –æ—Ç Groq Llama.'

  const chatResponse: ChatResponse = {
    content: content.trim(),
    mode,
    processingTime: 0,
    model: `Groq: ${model}`
  }

  if (mode === 'deep') {
    chatResponse.thinking = generateThinkingProcess(message, content)
  }

  return chatResponse
}

// Cohere (Command –º–æ–¥–µ–ª–∏)
async function handleCohereRequest(
  message: string, 
  mode: 'fast' | 'deep',
  chatHistory: Array<{ role: 'user' | 'assistant', content: string }>
): Promise<ChatResponse> {
  const COHERE_API_URL = 'https://api.cohere.ai/v1/chat'
  const COHERE_TOKEN = process.env.COHERE_API_KEY
  
  if (!COHERE_TOKEN) {
    throw new Error('COHERE_API_KEY –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω. –ü–æ–ª—É—á–∏—Ç–µ –±–µ—Å–ø–ª–∞—Ç–Ω—ã–π —Ç–æ–∫–µ–Ω –Ω–∞ https://dashboard.cohere.ai')
  }

  // Cohere Command –º–æ–¥–µ–ª–∏
  const model = mode === 'fast' 
    ? 'command-light'  // –ë—ã—Å—Ç—Ä–∞—è –º–æ–¥–µ–ª—å
    : 'command'        // –ö–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å
  
  // –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –∏—Å—Ç–æ—Ä–∏—é –¥–ª—è Cohere
  const chat_history = chatHistory.slice(-6).map(msg => ({
    role: msg.role === 'user' ? 'USER' : 'CHATBOT',
    message: msg.content
  }))

  safeLogger.info(`üîÆ –ò—Å–ø–æ–ª—å–∑—É–µ–º Cohere: ${model}`)

  const response = await fetch(COHERE_API_URL, {
    method: 'POST',
    headers: {
      'Authorization': `Bearer ${COHERE_TOKEN}`,
      'Content-Type': 'application/json',
    },
    body: JSON.stringify({
      model,
      message,
      chat_history,
      max_tokens: mode === 'fast' ? 200 : 500,
      temperature: mode === 'fast' ? 0.7 : 0.8,
      k: 40,
      p: 0.9
    })
  })

  if (!response.ok) {
    const errorData = await response.json().catch(() => ({}))
    throw new Error(`Cohere API error: ${response.status} - ${errorData.message || 'Unknown error'}`)
  }

  const result = await response.json()
  const content = result.text || '–ò–∑–≤–∏–Ω–∏—Ç–µ, –Ω–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –æ—Ç–≤–µ—Ç –æ—Ç Cohere.'

  const chatResponse: ChatResponse = {
    content: content.trim(),
    mode,
    processingTime: 0,
    model: `Cohere: ${model}`
  }

  if (mode === 'deep') {
    chatResponse.thinking = generateThinkingProcess(message, content)
  }

  return chatResponse
}

// OpenAI API (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
async function handleOpenAIRequest(
  message: string, 
  mode: 'fast' | 'deep',
  chatHistory: Array<{ role: 'user' | 'assistant', content: string }>
): Promise<ChatResponse> {
  const OPENAI_TOKEN = process.env.OPENAI_API_KEY
  
  if (!OPENAI_TOKEN) {
    throw new Error('OPENAI_API_KEY –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω')
  }

  const model = mode === 'fast' ? 'gpt-3.5-turbo' : 'gpt-4'
  
  const messages = [
    {
      role: 'system',
      content: createContextualPrompt(mode)
    },
    ...chatHistory.slice(-8),
    { role: 'user', content: message }
  ]

  const response = await fetch('https://api.openai.com/v1/chat/completions', {
    method: 'POST',
    headers: {
      'Authorization': `Bearer ${OPENAI_TOKEN}`,
      'Content-Type': 'application/json',
    },
    body: JSON.stringify({
      model,
      messages,
      max_tokens: mode === 'fast' ? 150 : 500,
      temperature: mode === 'fast' ? 0.7 : 0.8,
    })
  })

  if (!response.ok) {
    throw new Error(`OpenAI API error: ${response.status}`)
  }

  const result = await response.json()
  const content = result.choices[0]?.message?.content || '–ò–∑–≤–∏–Ω–∏—Ç–µ, –Ω–µ —É–¥–∞–ª–æ—Å—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–≤–µ—Ç.'

  const chatResponse: ChatResponse = {
    content,
    mode,
    processingTime: 0,
    model: `OpenAI: ${model}`
  }

  if (mode === 'deep') {
    chatResponse.thinking = generateThinkingProcess(message, content)
  }

  return chatResponse
}

// –¢–µ—Å—Ç–æ–≤–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è (—É–º–Ω–∞—è mock-—Å–∏—Å—Ç–µ–º–∞)
async function handleMockRequest(
  message: string, 
  mode: 'fast' | 'deep',
  chatHistory: Array<{ role: 'user' | 'assistant', content: string }>
): Promise<ChatResponse> {
  // –ò–º–∏—Ç–∏—Ä—É–µ–º –∑–∞–¥–µ—Ä–∂–∫—É
  await new Promise(resolve => setTimeout(resolve, mode === 'fast' ? 500 : 1500))

  // –ò—Å–ø–æ–ª—å–∑—É–µ–º —É–º–Ω—É—é –ª–æ–≥–∏–∫—É –æ—Ç–≤–µ—Ç–æ–≤
  const content = generateSmartMockResponse(message, mode)

  const chatResponse: ChatResponse = {
    content,
    mode,
    processingTime: 0,
    model: 'Komair Smart Mock'
  }

  if (mode === 'deep') {
    chatResponse.thinking = generateThinkingProcess(message, content)
  }

  return chatResponse
}

// –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–æ—Ü–µ—Å—Å–∞ –º—ã—à–ª–µ–Ω–∏—è –¥–ª—è –≥–ª—É–±–æ–∫–æ–≥–æ —Ä–µ–∂–∏–º–∞
// –ü—Ä–æ—Å—Ç–∞—è mock-—Å–∏—Å—Ç–µ–º–∞
function generateSmartMockResponse(question: string, mode: 'fast' | 'deep'): string {
  // Mock-–ø—Ä–æ–≤–∞–π–¥–µ—Ä –≤—Å–µ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã
  return generateUniversalResponse(question, mode)
}


// –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–æ—Ü–µ—Å—Å–∞ –º—ã—à–ª–µ–Ω–∏—è –¥–ª—è –≥–ª—É–±–æ–∫–æ–≥–æ —Ä–µ–∂–∏–º–∞
function generateThinkingProcess(question: string, answer: string): string {
  return `–ê–Ω–∞–ª–∏–∑ –≤–æ–ø—Ä–æ—Å–∞: "${question}"

üîç –®–∞–≥ 1: –ü–æ–Ω–∏–º–∞–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å —Å–ø—Ä–∞—à–∏–≤–∞–µ—Ç –æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π —Ç–µ–º–µ.

üß† –®–∞–≥ 2: –ê–Ω–∞–ª–∏–∑
–†–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—é –∫–ª—é—á–µ–≤—ã–µ –∞—Å–ø–µ–∫—Ç—ã –≤–æ–ø—Ä–æ—Å–∞.

üí° –®–∞–≥ 3: –†–µ—à–µ–Ω–∏–µ
–§–æ—Ä–º–∏—Ä—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç.

‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç: –ü–æ–ª—É—á–µ–Ω –¥–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç.`
}



// –£–ë–ò–†–ê–ï–ú –£–ù–ò–í–ï–†–°–ê–õ–¨–ù–´–ï –û–¢–í–ï–¢–´ - –°–ò–°–¢–ï–ú–ê –î–û–õ–ñ–ù–ê –†–ê–ë–û–¢–ê–¢–¨ –° –†–ï–ê–õ–¨–ù–´–ú–ò API!
// –≠—Ç–∞ —Ñ—É–Ω–∫—Ü–∏—è –±–æ–ª—å—à–µ –Ω–µ –¥–æ–ª–∂–Ω–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è –¥–ª—è fallback
function generateUniversalResponse(question: string, mode: 'fast' | 'deep'): string {
  // –≠—Ç–æ–π —Ñ—É–Ω–∫—Ü–∏–∏ –Ω–µ –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å –≤ production –∫–æ–¥–µ!
  // –ï—Å–ª–∏ –º—ã –∑–¥–µ—Å—å - –∑–Ω–∞—á–∏—Ç –≤—Å–µ —Ä–µ–∞–ª—å–Ω—ã–µ –ø—Ä–æ–≤–∞–π–¥–µ—Ä—ã –Ω–µ —Ä–∞–±–æ—Ç–∞—é—Ç
  throw new Error(`–ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê: –í—Å–µ AI –ø—Ä–æ–≤–∞–π–¥–µ—Ä—ã –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã! –í–æ–ø—Ä–æ—Å: ${question}, –†–µ–∂–∏–º: ${mode}`)
}